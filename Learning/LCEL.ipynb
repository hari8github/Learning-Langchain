{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c5513c",
   "metadata": {},
   "source": [
    "Traditional Chain vs LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0a2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"give a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e20730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model = \"llama3-8b-8192\", temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29342b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 14, 'total_tokens': 18, 'completion_time': 0.003508381, 'prompt_time': 0.0027012, 'queue_time': 0.271499214, 'total_time': 0.006209581}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_59f3b579f6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3caeaf82-bcc8-48f2-bafb-2d36981867c2-0', usage_metadata={'input_tokens': 14, 'output_tokens': 4, 'total_tokens': 18})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"hi da mendal\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b423f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec61095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbd4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harivenkat\\AppData\\Local\\Temp\\ipykernel_20324\\1783041939.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    "    output_parser = output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e7d943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '**Retrieval-Augmented Generation (RAG)**\\n\\nRetrieval-Augmented Generation (RAG) is a type of language model that combines the strengths of both retrieval-based and generation-based approaches to natural language processing (NLP). In this report, we will discuss the concept of RAG, its benefits, and its applications.\\n\\n**What is Retrieval-Augmented Generation?**\\n\\nRetrieval-Augmented Generation is a hybrid approach that leverages the strengths of both retrieval-based and generation-based models. In traditional generation-based models, the model generates text from scratch, whereas in retrieval-based models, the model retrieves relevant text from a database or knowledge graph. RAG combines these two approaches by first retrieving relevant text from a database and then generating new text based on the retrieved information.\\n\\n**How does RAG work?**\\n\\nThe RAG model consists of two main components:\\n\\n1. **Retrieval Component**: This component retrieves relevant text from a database or knowledge graph based on a given prompt or query. The retrieved text is used as input to the generation component.\\n2. **Generation Component**: This component generates new text based on the retrieved text. The generation component can be a traditional language model, such as a transformer-based model, or a more advanced model that incorporates attention mechanisms and other techniques.\\n\\n**Benefits of RAG**\\n\\nRAG offers several benefits over traditional generation-based and retrieval-based models:\\n\\n1. **Improved accuracy**: RAG can generate more accurate text by leveraging the strengths of both retrieval-based and generation-based models.\\n2. **Increased diversity**: RAG can generate more diverse text by combining the strengths of both models.\\n3. **Reduced data requirements**: RAG can generate high-quality text with less data than traditional generation-based models.\\n\\n**Applications of RAG**\\n\\nRAG has several applications in NLP, including:\\n\\n1. **Text summarization**: RAG can be used to generate summaries of long documents or articles.\\n2. **Question answering**: RAG can be used to generate answers to questions based on a given prompt or query.\\n3. **Language translation**: RAG can be used to generate translations of text from one language to another.\\n4. **Content generation**: RAG can be used to generate content for websites, social media, and other online platforms.\\n\\n**Challenges and Limitations**\\n\\nWhile RAG offers several benefits, it also has some challenges and limitations:\\n\\n1. **Data quality**: The quality of the retrieved text can impact the quality of the generated text.\\n2. **Retrieval bias**: The retrieval component may introduce bias in the generated text.\\n3. **Generation quality**: The generation component may not always generate high-quality text.\\n\\n**Conclusion**\\n\\nRetrieval-Augmented Generation is a promising approach that combines the strengths of both retrieval-based and generation-based models. RAG offers several benefits, including improved accuracy, increased diversity, and reduced data requirements. While RAG has several applications in NLP, it also has some challenges and limitations that need to be addressed.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8f9865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval-Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a type of language model that combines the strengths of both retrieval-based and generation-based approaches to natural language processing (NLP). In this report, we will discuss the concept of RAG, its benefits, and its applications.\n",
       "\n",
       "**What is Retrieval-Augmented Generation?**\n",
       "\n",
       "Retrieval-Augmented Generation is a hybrid approach that leverages the strengths of both retrieval-based and generation-based models. In traditional generation-based models, the model generates text from scratch, whereas in retrieval-based models, the model retrieves relevant text from a database or knowledge graph. RAG combines these two approaches by first retrieving relevant text from a database and then generating new text based on the retrieved information.\n",
       "\n",
       "**How does RAG work?**\n",
       "\n",
       "The RAG model consists of two main components:\n",
       "\n",
       "1. **Retrieval Component**: This component retrieves relevant text from a database or knowledge graph based on a given prompt or query. The retrieved text is used as input to the generation component.\n",
       "2. **Generation Component**: This component generates new text based on the retrieved text. The generation component can be a traditional language model, such as a transformer-based model, or a more advanced model that incorporates attention mechanisms and other techniques.\n",
       "\n",
       "**Benefits of RAG**\n",
       "\n",
       "RAG offers several benefits over traditional generation-based and retrieval-based models:\n",
       "\n",
       "1. **Improved accuracy**: RAG can generate more accurate text by leveraging the strengths of both retrieval-based and generation-based models.\n",
       "2. **Increased diversity**: RAG can generate more diverse text by combining the strengths of both models.\n",
       "3. **Reduced data requirements**: RAG can generate high-quality text with less data than traditional generation-based models.\n",
       "\n",
       "**Applications of RAG**\n",
       "\n",
       "RAG has several applications in NLP, including:\n",
       "\n",
       "1. **Text summarization**: RAG can be used to generate summaries of long documents or articles.\n",
       "2. **Question answering**: RAG can be used to generate answers to questions based on a given prompt or query.\n",
       "3. **Language translation**: RAG can be used to generate translations of text from one language to another.\n",
       "4. **Content generation**: RAG can be used to generate content for websites, social media, and other online platforms.\n",
       "\n",
       "**Challenges and Limitations**\n",
       "\n",
       "While RAG offers several benefits, it also has some challenges and limitations:\n",
       "\n",
       "1. **Data quality**: The quality of the retrieved text can impact the quality of the generated text.\n",
       "2. **Retrieval bias**: The retrieval component may introduce bias in the generated text.\n",
       "3. **Generation quality**: The generation component may not always generate high-quality text.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Retrieval-Augmented Generation is a promising approach that combines the strengths of both retrieval-based and generation-based models. RAG offers several benefits, including improved accuracy, increased diversity, and reduced data requirements. While RAG has several applications in NLP, it also has some challenges and limitations that need to be addressed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a150579",
   "metadata": {},
   "source": [
    "LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16eaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69597ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Retrieval-Augmented Generation (RAG)**\\n\\nRetrieval-Augmented Generation (RAG) is a type of language model that combines the strengths of both retrieval-based and generation-based approaches to natural language processing (NLP). In this report, we will discuss the concept of RAG, its benefits, and its applications.\\n\\n**What is Retrieval-Augmented Generation?**\\n\\nRetrieval-Augmented Generation is a hybrid approach that leverages the strengths of both retrieval-based and generation-based models. In traditional generation-based models, the model generates text from scratch, whereas in retrieval-based models, the model retrieves relevant text from a database or knowledge graph. RAG combines these two approaches by first retrieving relevant text from a database and then generating new text based on the retrieved information.\\n\\n**How does RAG work?**\\n\\nThe RAG model consists of two main components:\\n\\n1. **Retrieval Component**: This component retrieves relevant text from a database or knowledge graph based on a given prompt or query. The retrieved text is used as input to the generation component.\\n2. **Generation Component**: This component generates new text based on the retrieved text. The generation component can be a traditional language model, such as a transformer-based model, or a more advanced model that incorporates attention mechanisms and other techniques.\\n\\n**Benefits of RAG**\\n\\nRAG offers several benefits over traditional generation-based and retrieval-based models:\\n\\n1. **Improved accuracy**: RAG can generate more accurate text by leveraging the strengths of both retrieval-based and generation-based models.\\n2. **Increased diversity**: RAG can generate more diverse text by combining the strengths of both models.\\n3. **Reduced data requirements**: RAG can generate high-quality text with less data than traditional generation-based models.\\n\\n**Applications of RAG**\\n\\nRAG has several applications in NLP, including:\\n\\n1. **Text summarization**: RAG can be used to generate summaries of long documents or articles.\\n2. **Question answering**: RAG can be used to generate answers to questions based on a given prompt or query.\\n3. **Language translation**: RAG can be used to generate translations of text from one language to another.\\n4. **Content generation**: RAG can be used to generate content for websites, social media, and other online platforms.\\n\\n**Challenges and Limitations**\\n\\nWhile RAG offers several benefits, it also has some challenges and limitations:\\n\\n1. **Data quality**: The quality of the retrieved text can impact the quality of the generated text.\\n2. **Retrieval bias**: The retrieval component may introduce bias in the generated text.\\n3. **Generation quality**: The generation component may not always generate high-quality text.\\n\\n**Conclusion**\\n\\nRetrieval-Augmented Generation is a promising approach that combines the strengths of both retrieval-based and generation-based models. RAG offers several benefits, including improved accuracy, increased diversity, and reduced data requirements. While RAG has several applications in NLP, it also has some challenges and limitations that need to be addressed.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d399b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval-Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) is a type of language model that combines the strengths of both retrieval-based and generation-based approaches to natural language processing (NLP). In this report, we will discuss the concept of RAG, its benefits, and its applications.\n",
       "\n",
       "**What is Retrieval-Augmented Generation?**\n",
       "\n",
       "Retrieval-Augmented Generation is a hybrid approach that leverages the strengths of both retrieval-based and generation-based models. In traditional generation-based models, the model generates text from scratch, whereas in retrieval-based models, the model retrieves relevant text from a database or knowledge graph. RAG combines these two approaches by first retrieving relevant text from a database and then generating new text based on the retrieved information.\n",
       "\n",
       "**How does RAG work?**\n",
       "\n",
       "The RAG model consists of two main components:\n",
       "\n",
       "1. **Retrieval Component**: This component retrieves relevant text from a database or knowledge graph based on a given prompt or query. The retrieved text is used as input to the generation component.\n",
       "2. **Generation Component**: This component generates new text based on the retrieved text. The generation component can be a traditional language model, such as a transformer-based model, or a more advanced model that incorporates attention mechanisms and other techniques.\n",
       "\n",
       "**Benefits of RAG**\n",
       "\n",
       "RAG offers several benefits over traditional generation-based and retrieval-based models:\n",
       "\n",
       "1. **Improved accuracy**: RAG can generate more accurate text by leveraging the strengths of both retrieval-based and generation-based models.\n",
       "2. **Increased diversity**: RAG can generate more diverse text by combining the strengths of both models.\n",
       "3. **Reduced data requirements**: RAG can generate high-quality text with less data than traditional generation-based models.\n",
       "\n",
       "**Applications of RAG**\n",
       "\n",
       "RAG has several applications in NLP, including:\n",
       "\n",
       "1. **Text summarization**: RAG can be used to generate summaries of long documents or articles.\n",
       "2. **Question answering**: RAG can be used to generate answers to questions based on a given prompt or query.\n",
       "3. **Language translation**: RAG can be used to generate translations of text from one language to another.\n",
       "4. **Content generation**: RAG can be used to generate content for websites, social media, and other online platforms.\n",
       "\n",
       "**Challenges and Limitations**\n",
       "\n",
       "While RAG offers several benefits, it also has some challenges and limitations:\n",
       "\n",
       "1. **Data quality**: The quality of the retrieved text can impact the quality of the generated text.\n",
       "2. **Retrieval bias**: The retrieval component may introduce bias in the generated text.\n",
       "3. **Generation quality**: The generation component may not always generate high-quality text.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Retrieval-Augmented Generation is a promising approach that combines the strengths of both retrieval-based and generation-based models. RAG offers several benefits, including improved accuracy, increased diversity, and reduced data requirements. While RAG has several applications in NLP, it also has some challenges and limitations that need to be addressed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cedd80",
   "metadata": {},
   "source": [
    "How does pipe operator work??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfdce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chain_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chain_func)\n",
    "\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f89f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_two(x):\n",
    "    return x * 2\n",
    "\n",
    "def exponentiate_five(x):\n",
    "    return x ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3923494",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "multiply_two_runnable = Runnable(multiply_two)\n",
    "exponentiate_five_runnable = Runnable(exponentiate_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac9e20ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537824"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(multiply_two_runnable).__or__(exponentiate_five_runnable)\n",
    "\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c271f",
   "metadata": {},
   "source": [
    "instead of __or__ now we can use | (pipe operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37c9de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537824"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcel_chain2 = add_five_runnable | multiply_two_runnable | exponentiate_five_runnable\n",
    "\n",
    "lcel_chain2.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256141bd",
   "metadata": {},
   "source": [
    "RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bf6cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "multiply_two_runnable = RunnableLambda(multiply_two)\n",
    "exponentiate_five_runnable = RunnableLambda(exponentiate_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbc94e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537824"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcel_chain_lambda = add_five_runnable | multiply_two_runnable | exponentiate_five_runnable\n",
    "\n",
    "lcel_chain_lambda.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65563a7e",
   "metadata": {},
   "source": [
    "Interesting work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "462d4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a3c5189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a small report on AI:\n",
       "\n",
       "**Introduction**\n",
       "\n",
       "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI has been a rapidly growing field in recent years, with applications in various industries, including healthcare, finance, transportation, and education.\n",
       "\n",
       "**History of AI**\n",
       "\n",
       "The concept of AI dates back to the 1950s, when computer scientists like Alan Turing and Marvin Minsky began exploring the possibility of creating machines that could think and learn like humans. The term \"Artificial Intelligence\" was coined in 1956 by John McCarthy. Since then, AI has evolved through several phases, including:\n",
       "\n",
       "1. Rule-based systems (1950s-1970s): AI systems were programmed with explicit rules to perform specific tasks.\n",
       "2. Expert systems (1970s-1980s): AI systems were designed to mimic human expertise in specific domains.\n",
       "3. Machine learning (1980s-1990s): AI systems began to learn from data and improve their performance over time.\n",
       "4. Deep learning (2000s-present): AI systems have become increasingly sophisticated, using neural networks and deep learning algorithms to analyze complex data.\n",
       "\n",
       "**Types of AI**\n",
       "\n",
       "There are several types of AI, including:\n",
       "\n",
       "1. **Narrow or Weak AI**: Designed to perform a specific task, such as playing chess or recognizing faces.\n",
       "2. **General or Strong AI**: Aims to replicate human intelligence and perform any intellectual task.\n",
       "3. **Superintelligence**: Exceeds human intelligence and has the potential to revolutionize many aspects of society.\n",
       "\n",
       "**Applications of AI**\n",
       "\n",
       "AI has numerous applications across various industries, including:\n",
       "\n",
       "1. **Healthcare**: AI-powered diagnostic tools, personalized medicine, and medical research.\n",
       "2. **Finance**: AI-powered trading platforms, risk management, and customer service.\n",
       "3. **Transportation**: Self-driving cars, traffic management, and logistics optimization.\n",
       "4. **Education**: AI-powered learning platforms, personalized education, and language translation.\n",
       "\n",
       "**Challenges and Concerns**\n",
       "\n",
       "While AI has the potential to revolutionize many industries, there are also concerns about:\n",
       "\n",
       "1. **Job displacement**: AI may displace certain jobs, particularly those that involve repetitive tasks.\n",
       "2. **Bias and fairness**: AI systems can perpetuate biases and unfairness if not designed with fairness in mind.\n",
       "3. **Privacy and security**: AI systems can pose significant privacy and security risks if not properly secured.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Artificial Intelligence has come a long way since its inception, and its applications are vast and varied. While there are challenges and concerns, AI has the potential to transform many industries and improve our lives in numerous ways. As AI continues to evolve, it is essential to address the challenges and concerns associated with its development and deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a4deeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"RAG\"\n",
    "new_word = \"full form venuma da unaku?\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd329955",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_fact_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10bb4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_fact_runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f192e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is Retrieval-Augmented Generation (full form venuma da unaku?)?**\n",
       "Retrieval-Augmented Generation (full form venuma da unaku?) is a type of language model that combines the strengths of both retrieval-based and generation-based approaches to language processing. In traditional language models, the model generates text from scratch, relying solely on its internal knowledge and patterns. In contrast, full form venuma da unaku? models retrieve relevant information from a large corpus or database and then use this information to generate text.\n",
       "**How does full form venuma da unaku? work?**\n",
       "The full form venuma da unaku? process typically involves the following steps:\n",
       "1. **Retrieval**: The model retrieves relevant information from a large corpus or database, such as a knowledge graph, a database of articles, or a collection of text snippets.\n",
       "2. **Ranking**: The retrieved information is ranked based on its relevance to the input prompt or query.\n",
       "3. **Generation**: The top-ranked information is used as input to a language generation model, which generates text based on this information.\n",
       "4. **Post-processing**: The generated text may undergo additional processing, such as editing, summarization, or ranking, to improve its quality and relevance.\n",
       "**Advantages of full form venuma da unaku?**\n",
       "full form venuma da unaku? models have several advantages over traditional language models:\n",
       "1. **Improved accuracy**: By leveraging external knowledge and information, full form venuma da unaku? models can generate more accurate and informative text.\n",
       "2. **Increased diversity**: full form venuma da unaku? models can generate a wider range of text styles, formats, and tones, as they can draw from a larger pool of information.\n",
       "3. **Better handling of out-of-vocabulary words**: full form venuma da unaku? models can retrieve information about out-of-vocabulary words, allowing them to generate text that is more accurate and natural-sounding.\n",
       "**Applications of full form venuma da unaku?**\n",
       "full form venuma da unaku? models have a wide range of applications, including:\n",
       "1. **Question answering**: full form venuma da unaku? models can be used to generate answers to questions by retrieving relevant information from a knowledge graph or database.\n",
       "2. **Text summarization**: full form venuma da unaku? models can be used to generate summaries of long documents or articles by retrieving key information and generating a concise summary.\n",
       "3. **Language translation**: full form venuma da unaku? models can be used to generate translations of text by retrieving relevant information from a bilingual corpus or database.\n",
       "4. **Content generation**: full form venuma da unaku? models can be used to generate content, such as articles, blog posts, or social media updates, by retrieving relevant information and generating text based on this information.\n",
       "**Challenges and limitations of full form venuma da unaku?**\n",
       "While full form venuma da unaku? models have many advantages, they also face several challenges and limitations, including:\n",
       "1. **Retrieval bias**: The retrieved information may be biased towards certain sources or perspectives, which can affect the accuracy and diversity of the generated text.\n",
       "2. **Information overload**: The retrieved information may be too extensive or complex, making it difficult for the model to generate coherent and relevant text.\n",
       "3. **Evaluation metrics**: Developing effective evaluation metrics for full form venuma da unaku? models is challenging, as it requires assessing the quality and relevance of both the retrieved information and the generated text.\n",
       "Overall, Retrieval-Augmented Generation is a promising approach to language processing that has the potential to improve the accuracy, diversity, and relevance of generated text. However, it also presents several challenges and limitations that need to be addressed through further research and development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"Retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d623f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab543128",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0ae088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76768e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfa2b7",
   "metadata": {},
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a70ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the DeepSeek-V3 LLM is a \"mixture of experts model\" with 671B parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = retrieval | prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke(\"what architecture does the model use\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892dce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
