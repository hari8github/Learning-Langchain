{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66d49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435f6621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0017b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the question based on the context below,                 }\\nif you cannot answer the question using the                     }--->  (Rules) For Our Prompt\\nprovided information answer with \"I don\\'t know\"                 }\\n\\nContext: Aurelio AI is an AI development studio                 }\\nfocused on the fields of Natural Language Processing (NLP)      }\\nand information retrieval using modern tooling                  }--->   Context AI has\\nsuch as Large Language Models (LLMs),                           }\\nvector databases, and LangChain.                                }\\n\\nQuestion: Does Aurelio AI do anything related to LangChain?     }--->   User Question\\n\\nAnswer:                                                         }--->   AI Answer\\nHere we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don\\'t know, then we also have context and question which are being passed into this similarly to paramaters in a function.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The below is an example of how a RAG prompt may look:\n",
    "\"\"\"\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e68cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4528fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\")\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be21268a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db99da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8173400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7907ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc2bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = \"llama3-8b-8192\"\n",
    "\n",
    "llm = ChatGroq(model = model, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b76ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Liverpool FC is one of England's most successful football clubs. \n",
    "In January 2024, Jurgen Klopp announced he would leave Liverpool at the \n",
    "end of the 2023/24 season after nearly nine years as manager. The club \n",
    "subsequently appointed Arne Slot from Feyenoord as their new manager \n",
    "starting from the 2024/25 season.\n",
    "\n",
    "The club recently won the 2024 Carabao Cup (League Cup) beating Chelsea \n",
    "in the final at Wembley, which was Klopp's final trophy with the club. \n",
    "During the 2023/24 season, Liverpool also competed in the Premier League, \n",
    "Europa League, and FA Cup, showing strong performances across multiple \n",
    "competitions.\n",
    "\n",
    "The team's current captain is Virgil van Dijk, who took over the \n",
    "captaincy after Jordan Henderson's departure to Al-Ettifaq in Saudi \n",
    "Arabia during the 2023 summer transfer window. Mohamed Salah remains \n",
    "one of the team's key players, despite strong interest from Saudi \n",
    "Arabian clubs in 2023.\"\"\"\n",
    "\n",
    "query = \"Who is Liverpool's current manager and when was the last trophy they won?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f47ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.invoke({\"query\": query, \"context\" : context})\n",
    "\n",
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\" : lambda x : x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667ec141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"According to the context, Liverpool's current manager is Arne Slot, who will start from the 2024/25 season. As for the last trophy they won, it was the 2024 Carabao Cup (League Cup), which they won under the management of Jurgen Klopp.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 282, 'total_tokens': 342, 'completion_time': 0.048796783, 'prompt_time': 0.057755403, 'queue_time': 0.459352908, 'total_time': 0.106552186}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8d98bb46-f80d-493f-870a-7f76d648a792-0', usage_metadata={'input_tokens': 282, 'output_tokens': 60, 'total_tokens': 342})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.invoke({\"query\": query, \"context\" : context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136657f3",
   "metadata": {},
   "source": [
    "### Few Shot Prompting\n",
    "\n",
    "Many State-of-the-Art (SotA) LLMs are incredible at instruction following. Meaning that it requires much less effort to get the intended output or behavior from these models than is the case for older LLMs and smaller LLMs.\n",
    "\n",
    "Before creating an example let's first see how to use LangChain's few shot prompting objects. We will provide multiple examples and we'll feed them in as sequential human and ai messages so we setup the template like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee96bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de631ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96d93da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "# here is the formatted prompt.\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2941487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Liverpool FC Manager and Recent Trophy Wins\n",
      "\n",
      "#### Current Manager\n",
      "* Arne Slot, appointed from Feyenoord, will take over as Liverpool's new manager starting from the 2024/25 season.\n",
      "\n",
      "#### Last Trophy Won\n",
      "* The 2024 Carabao Cup (League Cup), which Liverpool won by beating Chelsea in the final at Wembley, was Jurgen Klopp's final trophy with the club.\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b13ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Liverpool FC Manager and Recent Trophy Wins\n",
       "\n",
       "#### Current Manager\n",
       "* Arne Slot, appointed from Feyenoord, will take over as Liverpool's new manager starting from the 2024/25 season.\n",
       "\n",
       "#### Last Trophy Won\n",
       "* The 2024 Carabao Cup (League Cup), which Liverpool won by beating Chelsea in the final at Wembley, was Jurgen Klopp's final trophy with the club."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ddd647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the user\\'s query based on the context below,                 }\\nif you cannot answer the question using the                         }\\nprovided information answer with \"I don\\'t know\"                     }\\n                                                                    }--->  (Rules)\\nAlways answer in markdown format. When doing so please              }\\nprovide headers, short summaries, follow with bullet                }\\npoints, then conclude. Here are some examples:                      }\\n\\n\\nUser: Can you explain gravity?                                      }\\nAI: ## Gravity                                                      }\\n                                                                    }\\nGravity is one of the fundamental forces in the universe.           }\\n                                                                    }\\n### Discovery                                                       }--->  (Example 1)\\n                                                                    }\\n* Gravity was first discovered by...                                }\\n                                                                    }\\n**To conclude**, Gravity is a fascinating topic and has been...     }\\n                                                                    }\\n\\nUser: What is the capital of France?                                }\\nAI: ## France                                                       }\\n                                                                    }\\nThe capital of France is Paris.                                     }\\n                                                                    }--->  (Example 2)\\n### Origins                                                         }\\n                                                                    }\\n* The name Paris comes from the...                                  }\\n                                                                    }\\n**To conclude**, Paris is highly regarded as one of the...          }\\n\\nContext: {context}                                                  }--->  (Context)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Answer the user's query based on the context below,                 }\n",
    "if you cannot answer the question using the                         }\n",
    "provided information answer with \"I don't know\"                     }\n",
    "                                                                    }--->  (Rules)\n",
    "Always answer in markdown format. When doing so please              }\n",
    "provide headers, short summaries, follow with bullet                }\n",
    "points, then conclude. Here are some examples:                      }\n",
    "\n",
    "\n",
    "User: Can you explain gravity?                                      }\n",
    "AI: ## Gravity                                                      }\n",
    "                                                                    }\n",
    "Gravity is one of the fundamental forces in the universe.           }\n",
    "                                                                    }\n",
    "### Discovery                                                       }--->  (Example 1)\n",
    "                                                                    }\n",
    "* Gravity was first discovered by...                                }\n",
    "                                                                    }\n",
    "**To conclude**, Gravity is a fascinating topic and has been...     }\n",
    "                                                                    }\n",
    "\n",
    "User: What is the capital of France?                                }\n",
    "AI: ## France                                                       }\n",
    "                                                                    }\n",
    "The capital of France is Paris.                                     }\n",
    "                                                                    }--->  (Example 2)\n",
    "### Origins                                                         }\n",
    "                                                                    }\n",
    "* The name Paris comes from the...                                  }\n",
    "                                                                    }\n",
    "**To conclude**, Paris is highly regarded as one of the...          }\n",
    "\n",
    "Context: {context}                                                  }--->  (Context)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8268ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5d08f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt2 = FewShotChatMessagePromptTemplate(\n",
    "    \n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ea78460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: ## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: ## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = few_shot_prompt2.format()\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e26c34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt2,\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb7049e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Liverpool FC\n",
       "\n",
       "### Current Manager\n",
       "\n",
       "* Liverpool's current manager is Arne Slot, who will take over from the start of the 2024/25 season.\n",
       "\n",
       "### Last Trophy Won\n",
       "\n",
       "* Liverpool won the 2024 Carabao Cup (League Cup) under the management of Jurgen Klopp, who announced his departure from the club at the end of the 2023/24 season.\n",
       "* This was Klopp's final trophy with the club.\n",
       "\n",
       "**To conclude**, Liverpool has a rich history of success, and their recent Carabao Cup win is a testament to their continued competitiveness in the football world."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline2 = prompt_template2 | llm\n",
    "\n",
    "out2 = pipeline2.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bfd92",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting\n",
    "We'll take a look at one more commonly used prompting technique called chain of thought (CoT). CoT is a technique that encourages the LLM to think through the problem step by step before providing an answer. The idea being that by breaking down the problem into smaller steps, the LLM is more likely to arrive at the correct answer and we are less likely to see hallucinations.\n",
    "\n",
    "To implement CoT we don't need any specific LangChain objects, instead we are simply modifying how we instruct our LLM within the system prompt. We will ask the LLM to list the problems that need to be solved, to solve each problem individually, and then to arrive at the final answer.\n",
    "\n",
    "Let's first test our LLM without CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd54224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "\n",
    "Be a helpful assistant and answer the user's questions.\n",
    "\n",
    "You MUST answer the question directly without any other \n",
    "text or explanation\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe9486af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a30cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cd6c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\" , cot_system_prompt),\n",
    "    (\"user\", \"{query}\" )\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7fdec77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To answer this question, I'll break it down into smaller subproblems and solve each one individually.\n",
       "\n",
       "Subproblem 1: How many keystrokes are needed to type the numbers from 1 to 9?\n",
       "\n",
       "* The numbers from 1 to 9 are: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
       "* Each number requires 3 keystrokes (1 digit for the number and 2 digits for the decimal point)\n",
       "* Therefore, the total number of keystrokes needed to type the numbers from 1 to 9 is: 9 x 3 = 27\n",
       "\n",
       "Subproblem 2: How many keystrokes are needed to type the numbers from 10 to 99?\n",
       "\n",
       "* The numbers from 10 to 99 are: 10, 11, 12, ..., 98, 99\n",
       "* Each number requires 4 keystrokes (1 digit for the tens place, 1 digit for the ones place, and 2 digits for the decimal point)\n",
       "* Since there are 90 numbers in this range (10 to 99), the total number of keystrokes needed is: 90 x 4 = 360\n",
       "\n",
       "Subproblem 3: How many keystrokes are needed to type the numbers from 100 to 499?\n",
       "\n",
       "* The numbers from 100 to 499 are: 100, 101, 102, ..., 498, 499\n",
       "* Each number requires 5 keystrokes (1 digit for the hundreds place, 1 digit for the tens place, 1 digit for the ones place, and 2 digits for the decimal point)\n",
       "* Since there are 400 numbers in this range (100 to 499), the total number of keystrokes needed is: 400 x 5 = 2000\n",
       "\n",
       "Subproblem 4: How many keystrokes are needed to type the number 500?\n",
       "\n",
       "* The number 500 requires 3 keystrokes (1 digit for the hundreds place, 1 digit for the tens place, and 1 digit for the ones place)\n",
       "\n",
       "Now, let's add up the total number of keystrokes needed to type the numbers from 1 to 500:\n",
       "\n",
       "* 27 keystrokes for the numbers from 1 to 9\n",
       "* 360 keystrokes for the numbers from 10 to 99\n",
       "* 2000 keystrokes for the numbers from 100 to 499\n",
       "* 3 keystrokes for the number 500\n",
       "\n",
       "Total number of keystrokes needed to type the numbers from 1 to 500: 27 + 360 + 2000 + 3 = 2390\n",
       "\n",
       "Therefore, it takes 2390 keystrokes to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\" : query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6096e1c",
   "metadata": {},
   "source": [
    "### Finally, as mentioned most LLMs are now trained to use CoT prompting by default. So let's see what happens if we don't explicitly tell the LLM to use CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9af9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "280a770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What a great question!\n",
       "\n",
       "To calculate the number of keystrokes needed to type the numbers from 1 to 500, we need to consider the following:\n",
       "\n",
       "* Each digit (0-9) requires 1 keystroke.\n",
       "* The number of digits in each number increases as we go from 1 to 500.\n",
       "\n",
       "Here's a breakdown:\n",
       "\n",
       "* Numbers from 1 to 9 require 1 digit each, so 9 keystrokes.\n",
       "* Numbers from 10 to 99 require 2 digits each, so 90 keystrokes (9 x 2).\n",
       "* Numbers from 100 to 499 require 3 digits each, so 400 keystrokes (4 x 100).\n",
       "* The number 500 requires 3 digits, so 3 keystrokes.\n",
       "\n",
       "Adding all these up, we get:\n",
       "\n",
       "9 + 90 + 400 + 3 = 502 keystrokes\n",
       "\n",
       "So, it takes a total of 502 keystrokes to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d33b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
