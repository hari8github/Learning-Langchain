{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec77ee0",
   "metadata": {},
   "source": [
    "Streaming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ded695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = \"llama3-8b-8192\"\n",
    "\n",
    "llm = ChatGroq(model = model, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72de4345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36, 'completion_time': 0.021893305, 'prompt_time': 0.001891923, 'queue_time': 0.265354295, 'total_time': 0.023785228}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a5fd348a-f2ca-4cf7-8489-d0c0a480aabc-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hi\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd57cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Art|ificial| Intelligence| (|AI|)| refers| to| the| development| of| computer| systems| that| can| perform| tasks| that| would| typically| require| human| intelligence|,| such| as|:\n",
      "\n",
      "|1|.| Learning|:| AI| systems| can| learn| from| data| and| improve| their| performance| over| time|.\n",
      "|2|.| Problem|-solving|:| AI| systems| can| analyze| data| and| make| decisions| or| take| actions| based| on| that| data|.\n",
      "|3|.| Reason|ing|:| AI| systems| can| draw| conclusions| and| make| predictions| based| on| the| data| they| have| been| trained| on|.\n",
      "|4|.| Perception|:| AI| systems| can| interpret| and| understand| data| from| sensors|,| such| as| images|,| speech|,| or| text|.\n",
      "\n",
      "|AI| is| a| broad| field| that| encompasses| many| sub|fields|,| including|:\n",
      "\n",
      "|1|.| Machine| Learning| (|ML|):| a| type| of| AI| that| enables| machines| to| learn| from| data| without| being| explicitly| programmed|.\n",
      "|2|.| Deep| Learning| (|DL|):| a| type| of| ML| that| uses| neural| networks| to| analyze| data|.\n",
      "|3|.| Natural| Language| Processing| (|N|LP|):| a| type| of| AI| that| enables| computers| to| understand|,| interpret|,| and| generate| human| language|.\n",
      "|4|.| Computer| Vision|:| a| type| of| AI| that| enables| computers| to| interpret| and| understand| visual| data| from| images| and| videos|.\n",
      "\n",
      "|AI| is| being| used| in| a| wide| range| of| applications|,| including|:\n",
      "\n",
      "|1|.| Virtual| assistants|:| AI|-powered| virtual| assistants|,| such| as| Siri|,| Alexa|,| and| Google| Assistant|,| can| perform| tasks|,| answer| questions|,| and| provide| information|.\n",
      "|2|.| Image| recognition|:| AI|-powered| image| recognition| systems| can| identify| objects|,| people|,| and| animals| in| images| and| videos|.\n",
      "|3|.| Speech| recognition|:| AI|-powered| speech| recognition| systems| can| trans|cribe| spoken| language| into| text|.\n",
      "|4|.| Predict|ive| maintenance|:| AI|-powered| predictive| maintenance| systems| can| analyze| data| from| sensors| and| predict| when| equipment| is| likely| to| fail|.\n",
      "|5|.| Healthcare|:| AI| is| being| used| in| healthcare| to| analyze| medical| images|,| diagnose| diseases|,| and| develop| personalized| treatment| plans|.\n",
      "|6|.| Finance|:| AI| is| being| used| in| finance| to| analyze| data|,| make| predictions|,| and| automate| trading| decisions|.\n",
      "|7|.| Transportation|:| AI| is| being| used| in| transportation| to| develop| self|-driving| cars|,| optimize| traffic| flow|,| and| improve| logistics|.\n",
      "\n",
      "|The| benefits| of| AI| include|:\n",
      "\n",
      "|1|.| Increased| efficiency|:| AI| can| automate| repetitive| tasks| and| free| up| human| resources| for| more| strategic| work|.\n",
      "|2|.| Improved| accuracy|:| AI| can| analyze| large| amounts| of| data| and| make| predictions| with| high| accuracy|.\n",
      "|3|.| Enhanced| decision|-making|:| AI| can| provide| insights| and| recommendations| to| support| decision|-making|.\n",
      "|4|.| Cost| savings|:| AI| can| reduce| costs| by| autom|ating| tasks| and| improving| efficiency|.\n",
      "\n",
      "|However|,| AI| also| raises| ethical| concerns|,| such| as|:\n",
      "\n",
      "|1|.| Job| displacement|:| AI| may| dis|place| certain| jobs|,| particularly| those| that| involve| repetitive| tasks|.\n",
      "|2|.| Bias|:| AI| systems| can| perpet|uate| biases| in| the| data| they| are| trained| on|.\n",
      "|3|.| Privacy|:| AI| systems| may| collect| and| analyze| large| amounts| of| personal| data|.\n",
      "|4|.| Trust|:| AI| systems| may| be| perceived| as| un|trust|worthy| or| unpredictable|.\n",
      "\n",
      "|Overall|,| AI| has| the| potential| to| transform| many| industries| and| aspects| of| our| lives|,| but| it| also| requires| careful| consideration| of| its| ethical| implications|.||"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "async for token in llm.astream(\"What is AI?\"):\n",
    "    tokens.append(token)\n",
    "    print(token.content, end = \"|\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d2b88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--ffac469c-3944-4ca2-825a-a28bc9b859a5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4290d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Art', additional_kwargs={}, response_metadata={}, id='run--ffac469c-3944-4ca2-825a-a28bc9b859a5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8adbb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='Artificial Intelligence', additional_kwargs={}, response_metadata={}, id='run--ffac469c-3944-4ca2-825a-a28bc9b859a5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0] + tokens[1] + tokens[2] + tokens[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec9ff643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' IntelligenceificialArt', additional_kwargs={}, response_metadata={}, id='run--ffac469c-3944-4ca2-825a-a28bc9b859a5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[3] + tokens[2] + tokens[1] + tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e7f7f2",
   "metadata": {},
   "source": [
    "Streaming with Agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63384ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`. You MUST use this tool\n",
    "    to conclude the interaction.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e41671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, multiply, exponentiate, subtract, final_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac69952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided back to you. You MUST \"\n",
    "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
    "        \"DO NOT use the same tool more than once.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name = \"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d53ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
    "\n",
    "# define the agent runnable\n",
    "agent : RunnableSerializable = (\n",
    "    {\n",
    "        \"input\" : lambda x: x[\"input\"],\n",
    "        \"chat_history\" : lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\" : lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice = \"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aef8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            out = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                break\n",
    "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
    "            # add the tool output to the agent scratchpad\n",
    "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
    "            agent_scratchpad.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": action_str,\n",
    "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
    "            })\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {action_str}\")\n",
    "            count += 1\n",
    "        # add the final output to the chat history\n",
    "        final_answer = out.tool_calls[0][\"args\"]\n",
    "        # this is a dictionary, so we convert it to a string for compatibility with\n",
    "        # the chat history\n",
    "        final_answer_str = json.dumps(final_answer)\n",
    "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer_str)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return final_answer\n",
    "\n",
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd98492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The add tool returned 20\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>{\"tool_call\":{\"id\":\"final_answer\",\"function\":{\"name\":\"final_answer\"},\"parameters\":{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}}}</tool-use>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is 10 + 10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m, in \u001b[0;36mCustomAgentExecutor.invoke\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     28\u001b[0m agent_scratchpad \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iterations:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# invoke a step for the agent to generate a tool call\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magent_scratchpad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_scratchpad\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# if the tool call is the final answer tool, we stop\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3046\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3044\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3045\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3046\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3047\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:5434\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5427\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5429\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5432\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5433\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5435\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5437\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5438\u001b[0m     )\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    978\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    979\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 799\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    800\u001b[0m                 m,\n\u001b[0;32m    801\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    802\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    803\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    804\u001b[0m             )\n\u001b[0;32m    805\u001b[0m         )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1045\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1046\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_groq\\chat_models.py:557\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    553\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    556\u001b[0m }\n\u001b[1;32m--> 557\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, params)\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>{\"tool_call\":{\"id\":\"final_answer\",\"function\":{\"name\":\"final_answer\"},\"parameters\":{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}}}</tool-use>'}}"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c44f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ").configurable_fields(\n",
    "    callbacks=ConfigurableField(\n",
    "        id=\"callbacks\",\n",
    "        name=\"callbacks\",\n",
    "        description=\"A list of callbacks to use for streaming\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3727b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc4971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, queue: asyncio.Queue):\n",
    "        self.queue = queue\n",
    "        self.final_answer_seen = False\n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                # this means we're done\n",
    "                return\n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put new token in the queue.\"\"\"\n",
    "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # check for final_answer tool call\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    # this will allow the stream to end on the next `on_llm_end` call\n",
    "                    self.final_answer_seen = True\n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
    "        return\n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
    "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
    "        # this should only be used at the end of our agent execution, however LangChain\n",
    "        # will call this at the end of every tool call, not just the final tool call\n",
    "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
    "        # tool call\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else:\n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16126f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--a421c2ab-899e-4588-a930-ffaaadae9dae'\n",
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'eeyr2mjvv', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]} response_metadata={} id='run--a421c2ab-899e-4588-a930-ffaaadae9dae' tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'eeyr2mjvv', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': 'eeyr2mjvv', 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand'} id='run--a421c2ab-899e-4588-a930-ffaaadae9dae' usage_metadata={'input_tokens': 1373, 'output_tokens': 68, 'total_tokens': 1441}\n"
     ]
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "async def stream(query:str):\n",
    "    response = agent.with_config(\n",
    "        callbacks = [streamer]\n",
    "    )\n",
    "    async for token in response.astream({\n",
    "        \"input\" : query,\n",
    "        \"chat_history\" : [],\n",
    "        \"agent_scratchpad\": []\n",
    "    }):\n",
    "        tokens.append(token)\n",
    "        print(token, flush = True)\n",
    "\n",
    "await stream(\"what is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d066929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'eeyr2mjvv', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_343314801a', 'service_tier': 'on_demand'}, id='run--a421c2ab-899e-4588-a930-ffaaadae9dae', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'eeyr2mjvv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1373, 'output_tokens': 68, 'total_tokens': 1441}, tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': 'eeyr2mjvv', 'index': 0, 'type': 'tool_call_chunk'}])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = tokens[0]\n",
    "\n",
    "for token in tokens[1:]:\n",
    "    tk+=token\n",
    "\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b2ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            async def stream(query: str):\n",
    "                response = self.agent.with_config(\n",
    "                    callbacks=[streamer]\n",
    "                )\n",
    "                # we initialize the output dictionary that we will be populating with\n",
    "                # our streamed output\n",
    "                output = None\n",
    "                # now we begin streaming\n",
    "                async for token in response.astream({\n",
    "                    \"input\": query,\n",
    "                    \"chat_history\": self.chat_history,\n",
    "                    \"agent_scratchpad\": agent_scratchpad\n",
    "                }):\n",
    "                    if output is None:\n",
    "                        output = token\n",
    "                    else:\n",
    "                        # we can just add the tokens together as they are streamed and\n",
    "                        # we'll have the full response object at the end\n",
    "                        output += token\n",
    "                    if token.content != \"\":\n",
    "                        # we can capture various parts of the response object\n",
    "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
    "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                    if tool_calls:\n",
    "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
    "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
    "                        if tool_name:\n",
    "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
    "                        arg = tool_calls[0][\"function\"][\"arguments\"]\n",
    "                        if arg != \"\":\n",
    "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
    "                return AIMessage(\n",
    "                    content=output.content,\n",
    "                    tool_calls=output.tool_calls,\n",
    "                    tool_call_id=output.tool_calls[0][\"id\"]\n",
    "                )\n",
    "\n",
    "            tool_call = await stream(query=input)\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_call_id\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\",\n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history, we only add the \"answer\" field\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return tool_args\n",
    "\n",
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "425f3da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_calls: [{'index': 0, 'id': 'm0jr407hf', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]\n",
      "tool_name: add\n",
      "arg: {\"x\":10,\"y\":10}\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m queue \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mQueue()\n\u001b[0;32m      2\u001b[0m streamer \u001b[38;5;241m=\u001b[39m QueueCallbackHandler(queue)\n\u001b[1;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent_executor\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 10 + 10\u001b[39m\u001b[38;5;124m\"\u001b[39m, streamer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[20], line 63\u001b[0m, in \u001b[0;36mCustomAgentExecutor.invoke\u001b[1;34m(self, input, streamer, verbose)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AIMessage(\n\u001b[0;32m     58\u001b[0m         content\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[0;32m     59\u001b[0m         tool_calls\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mtool_calls,\n\u001b[0;32m     60\u001b[0m         tool_call_id\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mtool_calls[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     61\u001b[0m     )\n\u001b[1;32m---> 63\u001b[0m tool_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# add initial tool call to scratchpad\u001b[39;00m\n\u001b[0;32m     65\u001b[0m agent_scratchpad\u001b[38;5;241m.\u001b[39mappend(tool_call)\n",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m, in \u001b[0;36mCustomAgentExecutor.invoke.<locals>.stream\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# now we begin streaming\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mastream({\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent_scratchpad\n\u001b[0;32m     38\u001b[0m }):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m         output \u001b[38;5;241m=\u001b[39m token\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:5624\u001b[0m, in \u001b[0;36mRunnableBindingBase.astream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5617\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5618\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mastream\u001b[39m(\n\u001b[0;32m   5619\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5622\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 5624\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mastream(\n\u001b[0;32m   5625\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5626\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5627\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5628\u001b[0m     ):\n\u001b[0;32m   5629\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3464\u001b[0m, in \u001b[0;36mRunnableSequence.astream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minput_aiter\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Input]:\n\u001b[0;32m   3462\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(input_aiter(), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3465\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3446\u001b[0m, in \u001b[0;36mRunnableSequence.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3439\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3440\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   3441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3444\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3445\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 3446\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m   3447\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[0;32m   3449\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3450\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3451\u001b[0m     ):\n\u001b[0;32m   3452\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:2321\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2320\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2321\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(py_anext(iterator), context)\n\u001b[0;32m   2322\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2323\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3413\u001b[0m, in \u001b[0;36mRunnableSequence._atransform\u001b[1;34m(self, inputs, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3411\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3412\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39matransform(final_pipeline, config)\n\u001b[1;32m-> 3413\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   3414\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:5663\u001b[0m, in \u001b[0;36mRunnableBindingBase.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5656\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5657\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   5658\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5661\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 5663\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39matransform(\n\u001b[0;32m   5664\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5665\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5666\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5667\u001b[0m     ):\n\u001b[0;32m   5668\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py:1488\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1488\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:615\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    613\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m _normalize_messages(messages)\n\u001b[0;32m    614\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((_LC_ID_PREFIX, \u001b[38;5;28mstr\u001b[39m(run_manager\u001b[38;5;241m.\u001b[39mrun_id)))\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[0;32m    616\u001b[0m     input_messages,\n\u001b[0;32m    617\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    619\u001b[0m ):\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m run_id\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_groq\\chat_models.py:634\u001b[0m, in \u001b[0;36mChatGroq._astream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    633\u001b[0m default_chunk_class: \u001b[38;5;28mtype\u001b[39m[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    635\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    636\u001b[0m ):\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    638\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_streaming.py:147\u001b[0m, in \u001b[0;36mAsyncStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[_T]:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32md:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_streaming.py:193\u001b[0m, in \u001b[0;36mAsyncStream.__stream__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    191\u001b[0m                 message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 193\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[0;32m    194\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m    195\u001b[0m                 request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m    196\u001b[0m                 body\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    197\u001b[0m             )\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sse\u001b[38;5;241m.\u001b[39mevent}, cast_to\u001b[38;5;241m=\u001b[39mcast_to, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "\u001b[1;31mAPIError\u001b[0m: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."
     ]
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fc56943",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "out = await agent_executor.invoke(\"What is 10 + 10\", streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4aaeae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-169' coro=<CustomAgentExecutor.invoke() done, defined at C:\\Users\\Harivenkat\\AppData\\Local\\Temp\\ipykernel_10376\\3754874902.py:19> exception=APIError(\"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Harivenkat\\AppData\\Local\\Temp\\ipykernel_10376\\3754874902.py\", line 63, in invoke\n",
      "    tool_call = await stream(query=input)\n",
      "  File \"C:\\Users\\Harivenkat\\AppData\\Local\\Temp\\ipykernel_10376\\3754874902.py\", line 34, in stream\n",
      "    async for token in response.astream({\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5624, in astream\n",
      "    async for item in self.bound.astream(\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3464, in astream\n",
      "    async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3446, in atransform\n",
      "    async for chunk in self._atransform_stream_with_config(\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2321, in _atransform_stream_with_config\n",
      "    chunk = await coro_with_context(py_anext(iterator), context)\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3413, in _atransform\n",
      "    async for output in final_pipeline:\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5663, in atransform\n",
      "    async for item in self.bound.atransform(\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1488, in atransform\n",
      "    async for output in self.astream(final, config, **kwargs):\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 615, in astream\n",
      "    async for chunk in self._astream(\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 634, in _astream\n",
      "    async for chunk in await self.async_client.create(\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_streaming.py\", line 147, in __aiter__\n",
      "    async for item in self._iterator:\n",
      "  File \"d:\\Langchain\\langchain_env\\lib\\site-packages\\groq\\_streaming.py\", line 193, in __stream__\n",
      "    raise APIError(\n",
      "groq.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--13852083-ec1f-455f-b29e-3abccc4b1571')\n",
      "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': '8ab2d2c5p', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={}, id='run--13852083-ec1f-455f-b29e-3abccc4b1571', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': '8ab2d2c5p', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\":10,\"y\":10}', 'id': '8ab2d2c5p', 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "generation_info={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand'}, id='run--13852083-ec1f-455f-b29e-3abccc4b1571', usage_metadata={'input_tokens': 1402, 'output_tokens': 68, 'total_tokens': 1470})\n",
      "<<STEP_END>>\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--b285bc78-489d-40d5-a971-f39904329a4b')\n",
      "message=AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': '7tg7jc01r', 'function': {'arguments': '{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}', 'name': 'final_answer'}, 'type': 'function'}]}, response_metadata={}, id='run--b285bc78-489d-40d5-a971-f39904329a4b', tool_calls=[{'name': 'final_answer', 'args': {'answer': 'The answer to 10 + 10 is 20.', 'tools_used': ['add']}, 'id': '7tg7jc01r', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'final_answer', 'args': '{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}', 'id': '7tg7jc01r', 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "generation_info={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_59f3b579f6', 'service_tier': 'on_demand'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_59f3b579f6', 'service_tier': 'on_demand'}, id='run--b285bc78-489d-40d5-a971-f39904329a4b', usage_metadata={'input_tokens': 1477, 'output_tokens': 49, 'total_tokens': 1526})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The answer to 10 + 10 is 20.', 'tools_used': ['add']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    print(token, flush=True)\n",
    "\n",
    "await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9db6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling add...\n",
      "{\"x\":10,\"y\":10}\n",
      "\n",
      "Calling final_answer...\n",
      "{\"answer\":\"The answer to 10 + 10 is 20.\",\"tools_used\":[\"add\"]}"
     ]
    }
   ],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "\n",
    "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    # first identify if we have a <<STEP_END>> token\n",
    "    if token == \"<<STEP_END>>\":\n",
    "        print(\"\\n\", flush=True)\n",
    "    # we'll first identify if the token is a tool call\n",
    "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
    "        # if we have a tool call with a tool name, we'll print it\n",
    "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
    "            print(f\"Calling {tool_name}...\", flush=True)\n",
    "        # if we have a tool call with arguments, we ad them to our args string\n",
    "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
    "            print(f\"{tool_args}\", end=\"\", flush=True)\n",
    "\n",
    "_ = await task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
